\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage{color}
%%\lstset{%frame=tb,
%	language=Python,
%%	aboveskip=3mm,
%%	belowskip=3mm,
%%	showstringspaces=false,
%%	columns=flexible,
%%	basicstyle={\small\ttfamily},
%%	numbers=none,
%%	numberstyle=\tiny\color{gray},
%	keywordstyle=\color{blue},
%	commentstyle=\color{dkgreen},
%	stringstyle=\color{mauve},
%%	breaklines=true,
%%	breakatwhitespace=true,
%%	tabsize=3
%}
\usepackage{geometry}
\geometry{left=10mm, right=10mm, top=10mm, bottom=15mm}
%\usepackage{fullpage}
\usepackage{graphicx}
\usepackage{float}
\usepackage{amsmath}
\usepackage{blindtext}
\graphicspath{ {./bilder/} }
\renewcommand{\baselinestretch}{2}
\author{Jan Odermatt}
\title{Zusammenfassung Machinelearning}
\begin{document}
\tableofcontents
\section{Machine Learning Grundlagen}
\includegraphics[width=0.4\textwidth]{disciplines_in_machine_learning.png}
\includegraphics[width=0.6\textwidth]{disciplines_matched.png}
\section{Data Quality Assessment}
	\begin{enumerate}
		\item Data Cleaning\footnote{Auch wenn die Datenqualität selbständig verbessert werden kann sollten: alle Änderungen dokumentiert werden, data-repository mit versionierung verwendet werden, den Herausgeber der Daten auf fehler in den Daten hinweisen}
		\begin{enumerate}
			\item Dublizierte Daten erkennen und entfernen
			\item Daten mit nullen können ersetzt werden.
			\item Daten Machine Learning freundlicher gestalten (z.B. für Farben eigene Zeilen erstellen, damit die Euklid-Distanz gerechnet werden kann.
		\end{enumerate}
		\item Analyse mit Hilfe von
		\begin{enumerate}
		\item 5 Nummer Zusammenfassung (median Q2, Quartile Q1 und Q3 sowie min und max)
		\item Boxplots um das Datenset auf Ausreisser zu prüfen.
		\item Varianz und Standardabweichung berechnen
		\end{enumerate}
	\end{enumerate}
\section{Machine Learning Fundamentals}
	\begin{tabular}{c c c}
	  Euklid Distanz & Kosinus Ähnlichkeit & Formel Kosinus Similarity\\
    	  \includegraphics[width=0.25\linewidth]{euclide_distance.png} & \includegraphics[width=0.25\linewidth]{cosine_similarity.png} & \includegraphics[width=0.25\linewidth]{cosine_similarity_formula.png} \\
	\end{tabular}
\section{Supervised Learning Basics}
\begin{tabular}{l l}
	\parbox[c]{0.3\textwidth}{\includegraphics[width=\linewidth]{true_positive}} &
	\begin{tabular}{l}
		$Accuracy = \frac{TP + TN}{Total}$ \\ $Errorrate = \frac{FP + FN}{Total}$ \\
		$Sensitivity = \frac{TP}{Actual Yes}$ = $\frac{TP}{TP + FN}$ \\ $Specificity = \frac{TN}{ActualNo} = \frac{TN}{TN + FP}$ \\ 
		$Precision = \frac{TP}{Predicted Yes} = \frac{TP}{TP + FP}$
	\end{tabular}
	
\end{tabular}
\section{Linear Regression}
	Das Modell hat generell die folgende Form: $y = h_\theta(x) = \theta_0 + \theta_1 x$.
	
	Mit $\bar{x}$ und $\bar{y}$ als Mittelwerte der Datenreihe, können somit die Werte $\theta_1$ und $\theta_0$ berechnet werden. \footnote{Bei $var^{(i)}$ ist $i$ ein Index für den Datenpunkt und kein Exponent.}
	\[ \theta_1 = \frac{\sum_{i=1}^{n}(y^{(i)}-\bar{y})(x^{(i)}-\bar{x})}{\sum_{i=1}^{n}(x^{(i)}-\bar{x})} = \frac{S_{xy}}{S_{xx}} \qquad
	\theta_0 = \bar{y}-\theta_1 \bar{x} \]

	Um die best möglichen $\theta_0$ und $\theta_1$ zu bekommen wird die Kostenfunktion
	\[ J(\theta_0,\theta_1)=\frac{1}{2 n} \sum_{i=1}^{n} [y^{(i)}-h_\theta(x^{(i)})]^2 \] minimiert, wobei $n$ für die Grösse des Trainingsets steht.
	Diese wird mit $\theta = (X^T X)^-1 X^T y$ umgesetzt.
	In python wird das mit X als $n \times m$ Matrix, bei der eine Spalte mit Einsen aufgefüllt wurde und 
	$\theta = 
	\begin{bmatrix}
	\theta_0 \\ \theta_1 \\ ... \\ \theta_m
	\end{bmatrix}$ definiert wird.
	\begin{lstlisting}
		theta = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)
	\end{lstlisting}

%$$SSE = \sum_{i=1}^{n} (e^{(i)}) = \sum_{i=1}^{n}[(y^{(i)}-\bar{y})-\theta_1 (x^{(i)}-\bar{x})]$$ 
%$$\frac{d}{d\theta_1}SSE = -2 S_xy + 2 \theta_1 S_{xx} = 0$$

	Korrelation liegt immer zwischen $-1 <= r <= 1$.
	$r = 0$ bedeutet keine Korrelation und $|r| = 1$ vollständige Korrelation.

\section{Gradient Descent}
\end{document}
